# LLM Server Dockerfile
# vLLM with NVIDIA CUDA support
# Requires: nvidia-docker runtime

FROM nvidia/cuda:12.1-devel-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.12 \
    python3.12-venv \
    python3-pip \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.12 as default
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1

# Upgrade pip
RUN python -m pip install --upgrade pip

# Install vLLM and dependencies
RUN pip install --no-cache-dir \
    vllm \
    transformers \
    accelerate \
    torch

# Expose API port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=60s --timeout=30s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:8080/v1/models || exit 1

# Default command (can be overridden in docker-compose)
CMD ["python", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", \
     "--quantization", "awq", \
     "--max-model-len", "8192", \
     "--gpu-memory-utilization", "0.95", \
     "--host", "0.0.0.0", \
     "--port", "8080"]
