# ===========================================
# Agentic Trading Application Configuration
# ===========================================
# Copy this file to .env and modify as needed
# cp .env.example .env

# -------------------------------------------
# LLM Configuration
# -------------------------------------------
# Provider: "vllm" (Windows/Linux GPU) or "ollama" (All platforms)
LLM_PROVIDER=vllm

# OpenAI-compatible API endpoint
# vLLM default: http://localhost:8080/v1
# Ollama default: http://localhost:11434/v1
LLM_BASE_URL=http://localhost:8080/v1

# Model name
# vLLM: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
# Ollama: deepseek-r1:7b, deepseek-r1:14b, llama3.1:70b, etc.
LLM_MODEL=deepseek-ai/DeepSeek-R1-Distill-Llama-70B

# Generation parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4096
LLM_TIMEOUT=120

# -------------------------------------------
# Market Data Configuration
# -------------------------------------------
# Mode: "live" (yfinance) or "mock" (local JSON data)
MARKET_DATA_MODE=live

# -------------------------------------------
# Redis Configuration (for state persistence)
# -------------------------------------------
REDIS_URL=redis://localhost:6379
REDIS_DB=0

# -------------------------------------------
# API Server Configuration
# -------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000

# -------------------------------------------
# Frontend Configuration
# -------------------------------------------
VITE_API_BASE_URL=http://localhost:8000/api
VITE_WS_BASE_URL=ws://localhost:8000/ws

# -------------------------------------------
# Environment
# -------------------------------------------
# Options: development, staging, production
ENVIRONMENT=development
DEBUG=true

# -------------------------------------------
# Platform-specific notes
# -------------------------------------------
#
# WINDOWS (RTX 3090):
#   LLM_PROVIDER=vllm
#   LLM_BASE_URL=http://localhost:8080/v1
#   LLM_MODEL=deepseek-ai/DeepSeek-R1-Distill-Llama-70B
#
# MACOS (M1 Pro):
#   LLM_PROVIDER=ollama
#   LLM_BASE_URL=http://localhost:11434/v1
#   LLM_MODEL=deepseek-r1:7b
#
# -------------------------------------------
