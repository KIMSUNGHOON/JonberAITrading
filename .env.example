# ===========================================
# Agentic Trading Application Configuration
# ===========================================
# Copy this file to .env and modify as needed
# cp .env.example .env

# -------------------------------------------
# LLM Configuration
# -------------------------------------------
# Provider: "ollama" (Windows/macOS) or "vllm" (Linux with NVIDIA GPU)
LLM_PROVIDER=ollama

# OpenAI-compatible API endpoint
# Ollama: http://localhost:11434/v1
# vLLM: http://localhost:8080/v1
LLM_BASE_URL=http://localhost:11434/v1

# Model name
# Ollama: deepseek-r1:32b, deepseek-r1:14b, deepseek-r1:7b
# vLLM: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B (AWQ)
LLM_MODEL=deepseek-r1:14b

# Generation parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4096
LLM_TIMEOUT=120

# -------------------------------------------
# Market Data Configuration
# -------------------------------------------
# Mode: "live" (yfinance) or "mock" (local JSON data)
MARKET_DATA_MODE=live

# -------------------------------------------
# Redis Configuration (for state persistence)
# -------------------------------------------
REDIS_URL=redis://localhost:6379
REDIS_DB=0

# -------------------------------------------
# API Server Configuration
# -------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000

# -------------------------------------------
# Frontend Configuration
# -------------------------------------------
VITE_API_BASE_URL=http://localhost:8000/api
VITE_WS_BASE_URL=ws://localhost:8000/ws

# -------------------------------------------
# Environment
# -------------------------------------------
# Options: development, staging, production
ENVIRONMENT=development
DEBUG=true

# -------------------------------------------
# Platform-specific examples
# -------------------------------------------
#
# WINDOWS (Ollama + RTX 3090 24GB):
#   LLM_PROVIDER=ollama
#   LLM_BASE_URL=http://localhost:11434/v1
#   LLM_MODEL=deepseek-r1:32b
#
# LINUX (vLLM + RTX 3090 24GB):
#   LLM_PROVIDER=vllm
#   LLM_BASE_URL=http://localhost:8080/v1
#   LLM_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
#
# MACOS (Ollama + M1 Pro 24GB):
#   LLM_PROVIDER=ollama
#   LLM_BASE_URL=http://localhost:11434/v1
#   LLM_MODEL=deepseek-r1:14b
#
# -------------------------------------------
